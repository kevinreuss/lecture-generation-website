<!DOCTYPE html>
<html>
  <head>
    <meta charset="UTF-8" />
    <title>Task Scheduling with Apache Airflow</title>
    <!-- Markdown Parser -->
    <script src="https://cdn.jsdelivr.net/npm/marked/marked.min.js"></script>
    <!-- Code Syntax Highlighting -->
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.8.0/styles/github.min.css" />
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.8.0/highlight.min.js"></script>
    <style>
      .markdown-content {
        max-width: 800px;
        margin: 0 auto;
        padding: 20px;
        font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto,
          "Helvetica Neue", Arial, sans-serif;
        line-height: 1.6;
      }
      pre code {
        border-radius: 4px;
        padding: 1em;
        background-color: #f6f8fa;
        display: block;
        overflow-x: auto;
      }
      h1, h2, h3 {
        color: #24292e;
        margin-top: 24px;
        margin-bottom: 16px;
      }
      a {
        color: #0366d6;
        text-decoration: none;
      }
      a:hover {
        text-decoration: underline;
      }
      #markdown-content {
        display: none;
      }
    </style>
  </head>
  <body>
    <div id="content" class="markdown-content"></div>
    <!-- Markdown Content -->
    <textarea id="markdown-content">
# Task Scheduling with Apache Airflow

Apache Airflow is a platform to programmatically author, schedule, and monitor workflows. Use it to author workflows as directed acyclic graphs (DAGs) of tasks. Here's how to define a simple DAG:

```python
from datetime import datetime
from airflow import DAG
from airflow.operators.dummy_operator import DummyOperator

dag = DAG('my_dag', start_date=datetime(2022, 1, 1))

start = DummyOperator(task_id='start', dag=dag)
end = DummyOperator(task_id='end', dag=dag)

start >> end
```
This DAG consists of two tasks (`start` and `end`). The `>>` operator signifies that `end` should run after `start`.

Airflow provides a rich set of built-in operators for many common tasks:

* `BashOperator` to execute bash commands
* `PythonOperator` to execute Python callables
* `EmailOperator` to send emails
* `SimpleHttpOperator` to send HTTP requests
* `MySqlOperator`, `SqliteOperator`, `PostgresOperator`, `MsSqlOperator`, `OracleOperator`, `JdbcOperator`, `RedshiftOperator` to execute SQL commands
* and many more

Here's a DAG that executes a Python function:

```python
from airflow.operators.python_operator import PythonOperator

def my_func():
    print("Hello, Airflow")

task = PythonOperator(task_id='my_task', python_callable=my_func, dag=dag)

start >> task >> end
```
The `PythonOperator` calls the `my_func` function when the task is run.

Tasks are instances of `BaseOperator`, and they're parameterized by a set of arguments and a collection of upstream and downstream tasks. You can use operators to express complex dependency structures between tasks, and Airflow will handle their scheduling and execution.

Airflow comes with a web-based user interface where you can monitor your workflows, retry failed tasks, and debug issues. You can also use it to trigger workflows manually or on a schedule.
    </textarea>
    <!-- Main Script -->
    <script>
      const markdown = document.getElementById("markdown-content").value;
      marked.setOptions({
        breaks: true,
        gfm: true,
        headerIds: true,
        highlight: function (code, lang) {
          if (lang && hljs.getLanguage(lang)) {
            return hljs.highlight(code, { language: lang }).value;
          }
          return hljs.highlightAuto(code).value;
        },
      });
      document.getElementById("content").innerHTML = marked.parse(markdown);
      hljs.highlightAll();
    </script>
  </body>
</html>